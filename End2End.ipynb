{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkCm6YAfIsDDoCE/G3syJ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Supun1234/Thesis/blob/main/End2End.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# I. SETUP: INSTALL NECESSARY LIBRARIES\n",
        "# ==============================================================================\n",
        "!pip install transformers torch spacy pandas -q\n",
        "!pip install spacy-transformers -q\n",
        "!python -m spacy download en_core_web_trf -q\n",
        "\n",
        "# ==============================================================================\n",
        "# II. IMPORT LIBRARIES AND LOAD MODELS\n",
        "# ==============================================================================\n",
        "import spacy\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "# --- Hugging Face Model (dslim/bert-base-NER) ---\n",
        "hf_tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "hf_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "hf_ner_pipeline = pipeline(\"ner\", model=hf_model, tokenizer=hf_tokenizer, aggregation_strategy=\"simple\")\n",
        "\n",
        "# --- spaCy Model (with Transformer Pipeline) ---\n",
        "spacy_nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "# ==============================================================================\n",
        "# III. CORE FUNCTIONS OF THE REQUIREMENT EXTRACTION PIPELINE\n",
        "# ==============================================================================\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Simple text preprocessing function.\"\"\"\n",
        "    return text.strip()\n",
        "\n",
        "def extract_agr_from_huggingface(sentence):\n",
        "    \"\"\"Extracts AGR from a sentence using Hugging Face NER.\"\"\"\n",
        "    ner_results = hf_ner_pipeline(sentence)\n",
        "    actor = {\"text\": None, \"confidence\": 0.0}\n",
        "    goal = {\"text\": None, \"confidence\": 0.0}\n",
        "    rationale = {\"text\": None, \"confidence\": 0.0}\n",
        "\n",
        "    # Heuristic: First PER or ORG is the Actor\n",
        "    for entity in ner_results:\n",
        "        if entity['entity_group'] in ['PER', 'ORG']:\n",
        "            actor[\"text\"] = entity['word']\n",
        "            actor[\"confidence\"] = float(entity['score'])\n",
        "            break\n",
        "\n",
        "    # Fallback for generic actors if NER fails\n",
        "    if not actor[\"text\"]:\n",
        "        generic_actors = ['user', 'users', 'admin', 'customer', 'system', 'application']\n",
        "        for act in generic_actors:\n",
        "            if act in sentence.lower():\n",
        "                actor[\"text\"] = act\n",
        "                actor[\"confidence\"] = 0.80\n",
        "                break\n",
        "\n",
        "    # Rationale & Goal Extraction with expanded keywords\n",
        "    rationale_keywords = ['so that', 'in order to', 'to', 'without']\n",
        "    text_to_split = sentence\n",
        "\n",
        "    found_rationale = False\n",
        "    for keyword in rationale_keywords:\n",
        "        if f\" {keyword} \" in text_to_split:\n",
        "            parts = text_to_split.split(f\" {keyword} \", 1)\n",
        "            goal[\"text\"] = parts[0].strip()\n",
        "            rationale[\"text\"] = (keyword + \" \" + parts[1]).strip()\n",
        "            goal[\"confidence\"] = 0.90\n",
        "            rationale[\"confidence\"] = 0.90\n",
        "            found_rationale = True\n",
        "            break\n",
        "\n",
        "    if not found_rationale:\n",
        "        goal[\"text\"] = text_to_split\n",
        "        goal[\"confidence\"] = 0.85\n",
        "\n",
        "    return {\"Actor\": actor, \"Goal\": goal, \"Rationale\": rationale}\n",
        "\n",
        "def extract_agr_from_spacy(sentence):\n",
        "    \"\"\"Extracts AGR using spaCy with robust dependency parsing.\"\"\"\n",
        "    doc = spacy_nlp(sentence)\n",
        "    actor = {\"text\": None, \"confidence\": 0.0}\n",
        "    goal = {\"text\": None, \"confidence\": 0.0}\n",
        "    rationale = {\"text\": None, \"confidence\": 0.0}\n",
        "\n",
        "    # 1. Actor Extraction (NER with grammatical subject fallback)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in [\"PERSON\", \"ORG\"]:\n",
        "            actor[\"text\"] = ent.text\n",
        "            actor[\"confidence\"] = 0.95\n",
        "            break\n",
        "    if not actor[\"text\"]:\n",
        "        for token in doc:\n",
        "            if \"nsubj\" in token.dep_:\n",
        "                subject_phrase = ' '.join([t.text for t in token.subtree])\n",
        "                actor[\"text\"] = subject_phrase\n",
        "                actor[\"confidence\"] = 0.90\n",
        "                break\n",
        "\n",
        "    # 2. Goal & Rationale Extraction\n",
        "    rationale_keywords = ['so that', 'in order to', 'to', 'without']\n",
        "    rationale_start_index = -1\n",
        "    for keyword in rationale_keywords:\n",
        "        if f\" {keyword} \" in sentence:\n",
        "            rationale_start_index = sentence.find(f\" {keyword} \")\n",
        "            break\n",
        "\n",
        "    if rationale_start_index != -1:\n",
        "        goal[\"text\"] = sentence[:rationale_start_index].strip()\n",
        "        rationale[\"text\"] = sentence[rationale_start_index:].strip()\n",
        "        goal[\"confidence\"] = 0.95\n",
        "        rationale[\"confidence\"] = 0.95\n",
        "    else:\n",
        "        goal[\"text\"] = sentence\n",
        "        goal[\"confidence\"] = 0.90\n",
        "\n",
        "    # 3. Refine Goal text by removing the actor\n",
        "    if actor[\"text\"] and goal[\"text\"] and actor[\"text\"] in goal[\"text\"]:\n",
        "        actor_end_index = goal[\"text\"].find(actor[\"text\"]) + len(actor[\"text\"])\n",
        "        refined_goal_text = goal[\"text\"][actor_end_index:].strip()\n",
        "        filler_words = [\"shall\", \"should\", \"must\", \"will\", \"can\"]\n",
        "        first_word = refined_goal_text.split(' ')[0] if refined_goal_text else \"\"\n",
        "        if first_word in filler_words:\n",
        "            refined_goal_text = refined_goal_text.replace(first_word, \"\", 1).strip()\n",
        "        goal[\"text\"] = refined_goal_text\n",
        "\n",
        "    return {\"Actor\": actor, \"Goal\": goal, \"Rationale\": rationale}\n",
        "\n",
        "def merge_agr_triplets(hf_agr, spacy_agr):\n",
        "    \"\"\"Merges AGR triplets, favoring the more robust spaCy output.\"\"\"\n",
        "    merged_agr = {}\n",
        "    def choose_best(slot_name):\n",
        "        hf_slot, spacy_slot = hf_agr[slot_name], spacy_agr[slot_name]\n",
        "        if spacy_slot[\"text\"]: return spacy_slot\n",
        "        if hf_slot[\"text\"]: return hf_slot\n",
        "        return spacy_slot\n",
        "    merged_agr[\"Actor\"] = choose_best(\"Actor\")\n",
        "    merged_agr[\"Goal\"] = choose_best(\"Goal\")\n",
        "    merged_agr[\"Rationale\"] = choose_best(\"Rationale\")\n",
        "    return merged_agr\n",
        "\n",
        "def evaluate_completeness_and_confidence(merged_agr):\n",
        "    \"\"\"Calculates completeness and confidence scores.\"\"\"\n",
        "    filled_slots = sum(1 for slot in merged_agr.values() if slot[\"text\"])\n",
        "    completeness = filled_slots / 3.0\n",
        "    c_actor = merged_agr[\"Actor\"][\"confidence\"] if merged_agr[\"Actor\"][\"text\"] else 0\n",
        "    c_goal = merged_agr[\"Goal\"][\"confidence\"] if merged_agr[\"Goal\"][\"text\"] else 0\n",
        "    c_rationale = merged_agr[\"Rationale\"][\"confidence\"] if merged_agr[\"Rationale\"][\"text\"] else 0\n",
        "    weighted_confidence = (c_actor + c_goal + c_rationale) / 3.0\n",
        "    return completeness, weighted_confidence\n",
        "\n",
        "# ==============================================================================\n",
        "# IV. OUTPUT AND VISUALIZATION FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def display_results(final_agr, completeness, confidence):\n",
        "    \"\"\"Displays the final structured requirement in a table.\"\"\"\n",
        "    display_data = {\n",
        "        'Actor': [final_agr[\"Actor\"][\"text\"]],\n",
        "        'Goal': [final_agr[\"Goal\"][\"text\"]],\n",
        "        'Rationale': [final_agr[\"Rationale\"][\"text\"]],\n",
        "        'Completeness': [f\"{completeness:.2%}\"],\n",
        "        'Confidence': [f\"{confidence:.2%}\"]\n",
        "    }\n",
        "    df = pd.DataFrame(display_data)\n",
        "    print(\"\\n--- Structured Requirement ---\")\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "def print_comma_separated_agr(final_agr):\n",
        "    \"\"\"\n",
        "    (NEW) Prints the extracted AGR components as a single comma-separated string.\n",
        "    \"\"\"\n",
        "    actor_text = final_agr['Actor']['text'] or \"None\"\n",
        "    goal_text = final_agr['Goal']['text'] or \"None\"\n",
        "    rationale_text = final_agr['Rationale']['text'] or \"None\"\n",
        "\n",
        "    # Create the comma-separated string\n",
        "    agr_string = f\"Actor: {actor_text}, Goal: {goal_text}, Rationale: {rationale_text}\"\n",
        "\n",
        "    print(\"\\n--- Comma-Separated AGR Result ---\")\n",
        "    print(agr_string)\n",
        "\n",
        "def print_semantic_graph(final_agr):\n",
        "    \"\"\"Prints a Cypher-like text representation of the semantic graph.\"\"\"\n",
        "    actor_node = f\"({final_agr['Actor']['text'] or 'UnspecifiedActor'})\"\n",
        "    goal_node = f\"({final_agr['Goal']['text'] or 'UnspecifiedGoal'})\"\n",
        "    cypher_string = f\"{actor_node} -[:PERFORMS_GOAL]-> {goal_node}\"\n",
        "    if final_agr['Rationale']['text']:\n",
        "        rationale_node = f\"({final_agr['Rationale']['text'] or 'UnspecifiedRationale'})\"\n",
        "        cypher_string += f\" -[:WITH_CONSTRAINT_OR_PURPOSE]-> {rationale_node}\"\n",
        "    print(\"\\n--- Semantic Graph (Cypher-like text) ---\")\n",
        "    print(cypher_string)\n",
        "\n",
        "# ==============================================================================\n",
        "# V. MAIN EXECUTION PIPELINE\n",
        "# ==============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the requirement extraction pipeline.\"\"\"\n",
        "    requirement_sentence = input(\"What is your requirement? \")\n",
        "    processed_sentence = preprocess_text(requirement_sentence)\n",
        "    print(\"\\nRunning NER Model 1 (Hugging Face with expanded keywords)...\")\n",
        "    hf_agr = extract_agr_from_huggingface(processed_sentence)\n",
        "    print(\"Running NER Model 2 (spaCy with dependency parsing)...\")\n",
        "    spacy_agr = extract_agr_from_spacy(processed_sentence)\n",
        "    print(\"Merging results...\")\n",
        "    final_agr = merge_agr_triplets(hf_agr, spacy_agr)\n",
        "    completeness, confidence = evaluate_completeness_and_confidence(final_agr)\n",
        "\n",
        "    # --- ALL OUTPUTS ---\n",
        "    display_results(final_agr, completeness, confidence) # 1. Table\n",
        "    print_comma_separated_agr(final_agr)                 # 2. Comma-separated string (NEW)\n",
        "    print_semantic_graph(final_agr)                      # 3. Graph\n",
        "\n",
        "# --- Run the main program ---\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trQsgDe2v9cJ",
        "outputId": "c8ccd5d8-66b8-47dd-fdc5-94037358125a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is your requirement? The system shall prevent unauthorized access to sensitive configuration files\n",
            "\n",
            "Running NER Model 1 (Hugging Face with expanded keywords)...\n",
            "Running NER Model 2 (spaCy with dependency parsing)...\n",
            "Merging results...\n",
            "\n",
            "--- Structured Requirement ---\n",
            "     Actor                        Goal                        Rationale Completeness Confidence\n",
            "The system prevent unauthorized access to sensitive configuration files      100.00%     93.33%\n",
            "\n",
            "--- Comma-Separated AGR Result ---\n",
            "Actor: The system, Goal: prevent unauthorized access, Rationale: to sensitive configuration files\n",
            "\n",
            "--- Semantic Graph (Cypher-like text) ---\n",
            "(The system) -[:PERFORMS_GOAL]-> (prevent unauthorized access) -[:WITH_CONSTRAINT_OR_PURPOSE]-> (to sensitive configuration files)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcrVbK0uxlD1",
        "outputId": "3f4c634d-b852-49be-9eb4-f1caf6a3bf14"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is your requirement? As an admin, I need to access the dashboard to monitor system performance in real time.\n",
            "\n",
            "Running NER Model 1 (Hugging Face with expanded keywords)...\n",
            "Running NER Model 2 (spaCy with dependency parsing)...\n",
            "Merging results...\n",
            "\n",
            "--- Structured Requirement ---\n",
            "Actor Goal                                                           Rationale Completeness Confidence\n",
            "    I need to access the dashboard to monitor system performance in real time.      100.00%     93.33%\n",
            "\n",
            "--- Comma-Separated AGR Result ---\n",
            "Actor: I, Goal: need, Rationale: to access the dashboard to monitor system performance in real time.\n",
            "\n",
            "--- Semantic Graph (Cypher-like text) ---\n",
            "(I) -[:PERFORMS_GOAL]-> (need) -[:WITH_CONSTRAINT_OR_PURPOSE]-> (to access the dashboard to monitor system performance in real time.)\n"
          ]
        }
      ]
    }
  ]
}